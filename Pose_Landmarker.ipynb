{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_cQX8dWu4Dv"
   },
   "source": [
    "# Retrieve numeric features representing the postures.\n",
    "\n",
    "In this notebook I convert the images to numeric representations, using posture detection from mediapipe. To convert the retrieved landmarks to features, I have used a method described by Siam et al.(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYKAJ5nDU8-I"
   },
   "source": [
    "### Retrieving the landmarks and edges for all images\n",
    "The function \"draw_landmarks_on_image\" takes an images and detection results as input and returns an annotaded image and save one to disk. Furthermore the model will be initialized. This code is copied from the MediaPipe example code: \n",
    "https://github.com/googlesamples/mediapipe/blob/main/examples/pose_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Pose_Landmarker.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s3E6NFV-00Qt"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, output_dir, image_name, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "  # To Save the image with annotations, first ensure the 'processed' directory exists\n",
    "  if not os.path.exists(output_dir):\n",
    "      os.makedirs(output_dir)\n",
    "\n",
    "  output_path = os.path.join(output_dir, image_name)\n",
    "  cv2.imwrite(output_path, annotated_image)\n",
    "\n",
    "#Create a PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='C:/Users/bcrui/Documents/OU-ML/pose_landmarker_full.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much shorter than the above code, but this does same as the function above, but for the YOLO code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "def draw_keypoints_on_image(rgb_image, output_dir, detection_result):\n",
    "    \n",
    "    # To Save the image with annotations, first ensure the 'processed' directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # View results\n",
    "    for r in detection_result:\n",
    "        im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "        im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image        \n",
    "        output_path = os.path.join(output_dir, rgb_image)\n",
    "        im.save(output_path)  # save image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the landmarks and Keypoints to features\n",
    "\n",
    "In this part I converted the retrieved landmarks and keypoints to features, according to the work of Siam et al.(2022)\n",
    "\n",
    "#### Point and vertices\n",
    "I used almost all landmarks, focusing specifically on points related to the torso, arms, and legs, while excluding facial landmarks, as the images do not feature the actors' faces. The left image shows the landmarks from Mediapipe and the image left the keypoints from Yolov8\n",
    "<table><tr>\n",
    "<td> <img src=\"https://developers.google.com/static/mediapipe/images/solutions/pose_landmarks_index.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"YoloKeypoints.png\" alt=\"Drawing\" style=\"width: 250px;\"/></br>\n",
    "source:https://learnopencv.com/wp-content/uploads/2021/05/fix-overlay-issue.jpg </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function \"calculate_angle_difference\" below, calculates the points and edges to angles of 2 connected edges, that can be saved as features for classification.\n",
    "\n",
    "The function \"Retrieve_the_angles\", calculates the angels of all 3 point combinations and will return a list with angles which can be used as features for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_angle_difference(x1, y1, x2, y2, x3, y3):\n",
    "    \n",
    "    #Calculate the difference between two angles formed by three points.\n",
    "\n",
    "    # Calculate each angle using the arctangent function\n",
    "    angle1 = math.atan2(y3 - y2, x3 - x2)\n",
    "    angle2 = math.atan2(y1 - y2, x1 - x2)\n",
    "\n",
    "    # Calculate the difference\n",
    "    angle_difference = angle1 - angle2\n",
    "\n",
    "    return angle_difference\n",
    "\n",
    "def Retrieve_the_angles(data, model):\n",
    "    \n",
    "    if model=='mediapipe':\n",
    "        #List of point combinations for mediapipe\n",
    "        points_sets =[(18,16,20),(22,16,14),(16,14,12),(17,15,19),(21,15,13),(15,13,11),(14,12,24),(13,11,23),(24,12,11),\n",
    "                    (12,11,23),(24,23,11),(23,24,12),(23,24,26),(25,23,24),(24,26,28),(23,25,27),(26,28,32),(30,32,28),(25,27,31),(29,31,27)]\n",
    "    else:\n",
    "         #List of point combinations for yolo\n",
    "        points_sets =[(10,9,6),(8,6,12),(12,6,5),(6,5,11),(11,5,7),(5,7,9),(6,12,11),(5,11,12),(14,12,11),(13,11,12),(12,14,16),(11,13,15)]\n",
    "    \n",
    "    #calculate for all point combinations the angle\n",
    "    angles=[]\n",
    "    for points in points_sets:\n",
    "        \n",
    "        #retrieve the points from the angle_list\n",
    "        p1 = points[0]\n",
    "        p2 = points[1]\n",
    "        p3 = points[2]\n",
    "\n",
    "        #calculate the angle difference and append it to the list angles\n",
    "        angle = calculate_angle_difference(data['x'][p1], data['y'][p1], data['x'][p2], data['y'][p2], data['x'][p3], data['y'][p3])\n",
    "        \n",
    "        # Check if the angle is less than 0, if so add 2*pi\n",
    "        if angle < 0:\n",
    "            angle += 2 * math.pi\n",
    "        angles.append(angle)  \n",
    "    \n",
    "    #return the angels\n",
    "    return angles\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Retrieve_feature' function retrieves the landmarks of an image and returns the features (the angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def Retrieve_features(rgb_image,images_dir,output_dir,model):\n",
    "    \n",
    "    #Load the input image.\n",
    "    image_path = os.path.join(images_dir, rgb_image)\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    \n",
    "    # Create a dictionary to hold the x and y coordinates\n",
    "    data = {\"x\": [], \"y\": []}\n",
    "    probs_media=[]\n",
    "    probs_yolo=[]\n",
    "    \n",
    "    #Select the correct model\n",
    "    if model==\"mediapipe\":\n",
    "        \n",
    "        #Detect pose landmarks from the input image.\n",
    "        detection_result = detector.detect(image)\n",
    "\n",
    "        # draw landmarks on image\n",
    "        annotated_image = draw_landmarks_on_image(image.numpy_view(), output_dir, rgb_image, detection_result)\n",
    "        \n",
    "        # Retrieve the list of landmarks\n",
    "        pose_landmarks = detection_result.pose_landmarks\n",
    "       \n",
    "        #if the landmarks are not null, retrieve the angles\n",
    "        if pose_landmarks==[]:\n",
    "            angles=[]\n",
    "        else: \n",
    "            # Flatten the list of lists if needed\n",
    "            if isinstance(pose_landmarks[0], list):\n",
    "                pose_landmarks = [item for sublist in pose_landmarks for item in sublist]\n",
    "\n",
    "            # Iterate over the landmarks and extract data\n",
    "            for landmark in pose_landmarks:\n",
    "                data[\"x\"].append(landmark.x)\n",
    "                data[\"y\"].append(landmark.y)\n",
    "                probs_media.append(float(landmark.visibility))\n",
    "\n",
    "            #retrieve the angles if the landmarks are above a certain treshold\n",
    "            if np.mean(probs_media)<0.80 and probs_media!=[]:\n",
    "                angles=[]\n",
    "            else:\n",
    "                angles = Retrieve_the_angles(data,'mediapipe') \n",
    "    else:\n",
    "        # Load a model\n",
    "        model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "        # Predict with the model\n",
    "        results = model(image_path,verbose=False) \n",
    "        \n",
    "        #create images with keypoints\n",
    "        draw_keypoints_on_image(rgb_image, output_dir, results)\n",
    "        \n",
    "        for r in results:\n",
    "            # Move the tensor to the CPU and convert it to a NumPy array, extract x,y and conf (probabilities of points)\n",
    "            coordinates = r.keypoints.xy.cpu().numpy()\n",
    "            probs_yolo.append(r.keypoints.conf.cpu().numpy())\n",
    "\n",
    "            # Extract x and y coordinates and probabilities for each point\n",
    "            x_coordinates = coordinates[0, :, 0]\n",
    "            y_coordinates = coordinates[0, :, 1]\n",
    "\n",
    "            # Create a dictionary with 'x' and 'y' keys\n",
    "            data = {'x': x_coordinates.tolist(), 'y': y_coordinates.tolist()}\n",
    "\n",
    "        #if the keypoints are not null and the probability of alle points nog below 0.8 retrieve the angles\n",
    "        if data==[] or (np.mean(probs_yolo)<0.80 and probs_yolo!=[]):\n",
    "            print(rgb_image)\n",
    "            angles=[]\n",
    "        else:\n",
    "            angles = Retrieve_the_angles(data,'yolo')\n",
    "    \n",
    "    return angles\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the BESST dataset and correct mistakes in the dataset\n",
    "First check for mistakes in the dataset of images that are absent in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images only in directory: {'009wN_45.jpg', '074mN_45.jpg', '076mW_90.jpg', '024wA_45.jpg'}\n",
      "Images only in variable: {'074mN_45.jp', '081mT_45.jpg', '024wA_45.jp', '009wN_45.jp', '076mW_90.jp'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Read the Excel file sheets\n",
    "postureData = pd.ExcelFile('datasets/BESST_Data.xlsx')\n",
    "posture_sheet1 = pd.read_excel(postureData, sheet_name= 'Frontal Bodies')\n",
    "posture_sheet1['Facing']='Frontal'\n",
    "posture_sheet2 = pd.read_excel(postureData, sheet_name= 'Averted Bodies')   \n",
    "posture_sheet2['Facing']='Averted'\n",
    "\n",
    "# Concatenate both sheets in a single dataframe\n",
    "posture_df = pd.concat([posture_sheet1,posture_sheet2], ignore_index=True)\n",
    "\n",
    "# set the directory with the images and for the processed images\n",
    "notebook_dir = os.getcwd()\n",
    "images_dir = Path(notebook_dir) / 'postures'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = os.listdir(images_dir )\n",
    "\n",
    "# Find images that are in the variable but not in the directory\n",
    "images_only_in_variable = set(posture_df['Image']) - set(all_files)\n",
    "\n",
    "# Find images that are in the variable but not in the directory\n",
    "images_only_in_directory = set(all_files) - set(posture_df['Image'])\n",
    "\n",
    "print(\"Images only in directory:\", images_only_in_directory)\n",
    "print(\"Images only in variable:\", images_only_in_variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct wrong filenames in the dataset and remove all the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images only in directory: set()\n",
      "Images only in variable: set()\n"
     ]
    }
   ],
   "source": [
    "# Replace '.jp' with '.jpg' in the 'Image' column\n",
    "posture_df['Image'] = posture_df['Image'].str.replace(r'\\.jp$', '.jpg', regex=True)\n",
    "\n",
    "# Remove rows where 'Image' is '081mT_45.jpg'\n",
    "posture_df = posture_df[posture_df['Image'] != '081mT_45.jpg']\n",
    "\n",
    "images_only_in_directory = set(all_files) - set(posture_df['Image'])\n",
    "\n",
    "# Iterate over the list of files which are not in the dataset\n",
    "for file_name in images_only_in_directory:\n",
    "    file_path = os.path.join(images_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Check one's more\n",
    "all_files = os.listdir(images_dir)\n",
    "images_only_in_variable = set(posture_df['Image']) - set(all_files)\n",
    "images_only_in_directory = set(all_files) - set(posture_df['Image'])\n",
    "\n",
    "print(\"Images only in directory:\", images_only_in_directory)\n",
    "print(\"Images only in variable:\", images_only_in_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving features for all images and adding to the dataset\n",
    "Add the features for each observation, to the dataset.\n",
    "\n",
    "On the printed pictures below (after running the code), no landmarks where found, so these were deleted from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bcrui\\AppData\\Local\\Temp\\ipykernel_31100\\76570754.py:8: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  features_media = posture_df['Image'].apply(lambda x: Retrieve_features(x,images_dir,output_media,'mediapipe')).apply(pd.Series)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "024wN_90.jpg\n",
      "057mA_90.jpg\n",
      "064mA_90.jpg\n",
      "006wA_90.jpg\n",
      "009wA_90.jpg\n",
      "021mE_90.jpg\n",
      "002wE_90.jpg\n",
      "008wF_90.jpg\n",
      "024wF_90.jpg\n",
      "038wF_90.jpg\n",
      "046wF_90.jpg\n",
      "047wF_90.jpg\n",
      "043mT_90.jpg\n",
      "005wT_90.jpg\n",
      "007wT_90.jpg\n",
      "065mU_90.jpg\n",
      "002wW_90.jpg\n",
      "006wN_45.jpg\n",
      "064mA_45.jpg\n",
      "016wA_45.jpg\n",
      "035wA_45.jpg\n",
      "047wA_45.jpg\n",
      "048wA_45.jpg\n",
      "021mE_45.jpg\n",
      "002wE_45.jpg\n",
      "053mF_45.jpg\n",
      "008wF_45.jpg\n",
      "009wF_45.jpg\n",
      "015wF_45.jpg\n",
      "041wF_45.jpg\n",
      "021mT_45.jpg\n",
      "054mT_45.jpg\n",
      "062mT_45.jpg\n",
      "075mT_45.jpg\n",
      "005wT_45.jpg\n",
      "022wT_45.jpg\n",
      "048wT_45.jpg\n",
      "049wT_45.jpg\n",
      "065mU_45.jpg\n",
      "071mU_45.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bcrui\\AppData\\Local\\Temp\\ipykernel_31100\\76570754.py:9: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  features_yolo = posture_df['Image'].apply(lambda x: Retrieve_features(x,images_dir,output_yolo,'yolo')).apply(pd.Series)\n"
     ]
    }
   ],
   "source": [
    "# set the directory with the images and for the processed images\n",
    "notebook_dir = os.getcwd()\n",
    "images_dir = Path(notebook_dir) / 'postures'\n",
    "output_media = Path(notebook_dir) / 'processed_media'\n",
    "output_yolo = Path(notebook_dir) / 'processed_yolo'\n",
    "\n",
    "# Use lambda to pass the column value and the fixed variables\n",
    "features_media = posture_df['Image'].apply(lambda x: Retrieve_features(x,images_dir,output_media,'mediapipe')).apply(pd.Series)\n",
    "features_yolo = posture_df['Image'].apply(lambda x: Retrieve_features(x,images_dir,output_yolo,'yolo')).apply(pd.Series)\n",
    "\n",
    "# Concatenate the results with the original DataFrame\n",
    "result_media = pd.concat([posture_df, features_media], axis=1)\n",
    "result_yolo = pd.concat([posture_df, features_yolo], axis=1)\n",
    "\n",
    "# Identify the rows where no landmarks were detected\n",
    "result_media = result_media.dropna()\n",
    "result_yolo = result_yolo.dropna()\n",
    "\n",
    "#save the new dataset to a csv file\n",
    "result_media.to_csv('datasets/postures_media.csv' ,index=False) \n",
    "result_yolo.to_csv('datasets/postures_yolo.csv' ,index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1069\n",
      "1088\n"
     ]
    }
   ],
   "source": [
    "print(result_media.shape[0])\n",
    "print(result_yolo.shape[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
